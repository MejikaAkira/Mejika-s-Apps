import os
import numpy as np
from typing import List, Dict, Any, Tuple, Optional
from openai import OpenAI
from dotenv import load_dotenv
from sklearn.metrics.pairwise import cosine_similarity
import json
import hashlib
import pickle

# ç’°å¢ƒå¤‰æ•°ã‚’èª­ã¿è¾¼ã¿
load_dotenv()

class VectorSearch:
    """
    embeddingãƒ•ã‚¡ã‚¤ãƒ«ï¼ˆembeddings.npy, embedding_ids.jsonï¼‰ã‚’ä½¿ã£ãŸæ„å‘³æ¤œç´¢å°‚ç”¨ã‚¯ãƒ©ã‚¹
    - DBã‚¢ã‚¯ã‚»ã‚¹ã¯è¡Œã‚ãšã€IDãƒªã‚¹ãƒˆã‚’è¿”ã™
    - embeddingãƒ•ã‚¡ã‚¤ãƒ«ãŒå­˜åœ¨ã—ãªã„å ´åˆã¯åˆæœŸåŒ–ã‚’ã‚¹ã‚­ãƒƒãƒ—ï¼ˆget_embeddingã®ã¿ä½¿ç”¨å¯èƒ½ï¼‰
    - ã‚¯ã‚¨ãƒªã‚­ãƒ£ãƒƒã‚·ãƒ¥æ©Ÿèƒ½ä»˜ã
    - å¤–éƒ¨ã‚¹ãƒˆãƒ¬ãƒ¼ã‚¸å¯¾å¿œ
    """
    def __init__(self, embedding_path: str = None, id_path: str = None, cache_path: str = None) -> None:
        api_key = os.getenv('OPENAI_API_KEY')
        if not api_key:
            raise ValueError("OPENAI_API_KEYãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚.envãƒ•ã‚¡ã‚¤ãƒ«ã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚")
        self.client = OpenAI(api_key=api_key)
        self.model = "text-embedding-3-small"
        
        # ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹ã®è¨­å®šï¼ˆç’°å¢ƒå¤‰æ•°å„ªå…ˆï¼‰
        self.embedding_path = embedding_path or os.getenv('EMBEDDING_PATH', 'embeddings.npy')
        self.id_path = id_path or os.getenv('EMBEDDING_IDS_PATH', 'embedding_ids.json')
        self.cache_path = cache_path or os.getenv('CACHE_PATH', 'query_cache.pkl')
        
        # ã‚¯ã‚¨ãƒªã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’ãƒ­ãƒ¼ãƒ‰
        self.query_cache = self._load_cache()
        
        # embeddingãƒ•ã‚¡ã‚¤ãƒ«ãŒå­˜åœ¨ã™ã‚‹å ´åˆã®ã¿ãƒ­ãƒ¼ãƒ‰
        if self._check_embedding_files():
            try:
                self.embeddings = np.load(self.embedding_path)
                with open(self.id_path, 'r', encoding='utf-8') as f:
                    self.id_list = json.load(f)
                if len(self.embeddings) != len(self.id_list):
                    raise ValueError("embeddings.npyã¨embedding_ids.jsonã®ä»¶æ•°ãŒä¸€è‡´ã—ã¾ã›ã‚“")
                self._embeddings_loaded = True
                print(f"âœ“ embeddingãƒ•ã‚¡ã‚¤ãƒ«ã‚’ãƒ­ãƒ¼ãƒ‰ã—ã¾ã—ãŸï¼ˆ{len(self.embeddings)}ä»¶ï¼‰")
                print(f"âœ“ ã‚¯ã‚¨ãƒªã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’ãƒ­ãƒ¼ãƒ‰ã—ã¾ã—ãŸï¼ˆ{len(self.query_cache)}ä»¶ï¼‰")
            except Exception as e:
                print(f"âš  embeddingãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ­ãƒ¼ãƒ‰ã«å¤±æ•—: {e}")
                self._embeddings_loaded = False
        else:
            print("âš  embeddingãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚get_embeddingãƒ¡ã‚½ãƒƒãƒ‰ã®ã¿ä½¿ç”¨å¯èƒ½ã§ã™ã€‚")
            print(f"  æœŸå¾…ã•ã‚Œã‚‹ãƒ‘ã‚¹: {self.embedding_path}, {self.id_path}")
            self._embeddings_loaded = False

    def _check_embedding_files(self) -> bool:
        """embeddingãƒ•ã‚¡ã‚¤ãƒ«ã®å­˜åœ¨ç¢ºèª"""
        return os.path.exists(self.embedding_path) and os.path.exists(self.id_path)

    def _load_cache(self) -> Dict[str, List[float]]:
        """ã‚¯ã‚¨ãƒªã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’ãƒ­ãƒ¼ãƒ‰"""
        if os.path.exists(self.cache_path):
            try:
                with open(self.cache_path, 'rb') as f:
                    return pickle.load(f)
            except Exception as e:
                print(f"âš  ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ­ãƒ¼ãƒ‰ã‚¨ãƒ©ãƒ¼: {e}")
        return {}

    def _save_cache(self) -> None:
        """ã‚¯ã‚¨ãƒªã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’ä¿å­˜"""
        try:
            with open(self.cache_path, 'wb') as f:
                pickle.dump(self.query_cache, f)
        except Exception as e:
            print(f"âš  ã‚­ãƒ£ãƒƒã‚·ãƒ¥ä¿å­˜ã‚¨ãƒ©ãƒ¼: {e}")

    def _get_cache_key(self, text: str) -> str:
        """ãƒ†ã‚­ã‚¹ãƒˆã‹ã‚‰ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚­ãƒ¼ã‚’ç”Ÿæˆ"""
        return hashlib.md5(text.encode('utf-8')).hexdigest()

    def get_embedding(self, text: str) -> List[float]:
        """
        ãƒ†ã‚­ã‚¹ãƒˆã‚’OpenAI APIã§embeddingï¼ˆãƒ™ã‚¯ãƒˆãƒ«åŒ–ï¼‰
        ã‚­ãƒ£ãƒƒã‚·ãƒ¥æ©Ÿèƒ½ä»˜ã
        """
        cache_key = self._get_cache_key(text)
        
        # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‹ã‚‰å–å¾—ã‚’è©¦è¡Œ
        if cache_key in self.query_cache:
            print(f"âœ“ ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‹ã‚‰embeddingã‚’å–å¾—: '{text[:20]}...'")
            return self.query_cache[cache_key]
        
        # APIå‘¼ã³å‡ºã—
        try:
            print(f"ğŸ”„ APIã‹ã‚‰embeddingã‚’å–å¾—: '{text[:20]}...'")
            response = self.client.embeddings.create(
                model=self.model,
                input=text
            )
            embedding = response.data[0].embedding
            
            # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã«ä¿å­˜
            self.query_cache[cache_key] = embedding
            self._save_cache()
            
            return embedding
        except Exception as e:
            print(f"ãƒ™ã‚¯ãƒˆãƒ«åŒ–ã‚¨ãƒ©ãƒ¼: {e}")
            raise

    def search(self, query: str, top_k: int = 10) -> List[Tuple[int, float]]:
        """
        ã‚¯ã‚¨ãƒªã«å¯¾ã—ã€embeddingãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰é¡ä¼¼åº¦é †ã«ä¸Šä½Nä»¶ã®(id, é¡ä¼¼åº¦)ã‚’è¿”ã™
        ã‚­ãƒ£ãƒƒã‚·ãƒ¥æ©Ÿèƒ½ä»˜ã
        """
        if not self._embeddings_loaded:
            raise ValueError("embeddingãƒ•ã‚¡ã‚¤ãƒ«ãŒãƒ­ãƒ¼ãƒ‰ã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚init_db.pyã‚’å®Ÿè¡Œã—ã¦ãã ã•ã„ã€‚")
        
        query_embedding = self.get_embedding(query)
        query_array = np.array(query_embedding).reshape(1, -1)
        # å…¨embeddingã¨ã‚³ã‚µã‚¤ãƒ³é¡ä¼¼åº¦è¨ˆç®—
        similarities = cosine_similarity(query_array, self.embeddings)[0]
        # ä¸Šä½top_kä»¶ã®ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’å–å¾—
        top_indices = similarities.argsort()[::-1][:top_k]
        # (ID, é¡ä¼¼åº¦)ã®ãƒªã‚¹ãƒˆã§è¿”ã™
        return [(self.id_list[i], float(similarities[i])) for i in top_indices]

    def get_cache_stats(self) -> Dict[str, Any]:
        """ã‚­ãƒ£ãƒƒã‚·ãƒ¥çµ±è¨ˆã‚’è¿”ã™"""
        return {
            'cached_queries': len(self.query_cache),
            'cache_size_mb': os.path.getsize(self.cache_path) / (1024 * 1024) if os.path.exists(self.cache_path) else 0
        }

    def get_embedding_info(self) -> Dict[str, Any]:
        """embeddingæƒ…å ±ã‚’è¿”ã™"""
        return {
            'embeddings_loaded': self._embeddings_loaded,
            'embedding_path': self.embedding_path,
            'id_path': self.id_path,
            'total_embeddings': len(self.embeddings) if self._embeddings_loaded else 0,
            'files_exist': self._check_embedding_files()
        } 